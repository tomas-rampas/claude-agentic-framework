---
category: mcp-integration
priority: high
agents: [maker-agent, reader-agent, debug-agent, security-agent, test-agent, docs-agent]
description: "Optimized filesystem MCP usage patterns"
tags: [filesystem, mcp, atomic-writes, large-files, performance]
last_updated: "2025-09-07"
mcp_dependencies: [filesystem]
---

# Filesystem Optimization Hook
# Optimizes filesystem MCP usage across all agents for maximum performance and reliability

hook_name: filesystem-optimization
version: "2.0"
trigger: "on_file_operation"

# CORE FILESYSTEM ACTIONS
actions:
  use_atomic_writes:
    description: "Implement atomic write operations for critical files"
    required: true
    mcp_server: "filesystem"
    implementation: |
      - Use filesystem.write() with atomic flag
      - Create temporary files for large operations
      - Implement rollback on write failure
      - Validate write completion before committing
    
    use_cases:
      - configuration_files
      - database_files
      - critical_application_state
      - shared_resources
  
  optimize_large_files:
    description: "Handle large file operations efficiently"
    required: true
    mcp_server: "filesystem"
    implementation: |
      - Use streaming operations for files > 10MB
      - Implement chunked reading/writing
      - Monitor memory usage during operations
      - Use progress indicators for user feedback
    
    thresholds:
      small_file: "< 1MB - Direct read/write"
      medium_file: "1MB - 10MB - Buffered operations"
      large_file: "10MB - 100MB - Streaming operations"
      huge_file: "> 100MB - Chunked streaming with progress"
  
  implement_caching:
    description: "Cache frequently accessed files and directory listings"
    required: false
    mcp_server: "filesystem"
    implementation: |
      - Cache directory listings
      - Cache file metadata
      - Implement TTL-based cache invalidation
      - Monitor cache hit rates
    
    cache_strategy:
      directory_listings:
        ttl: 300  # 5 minutes
        max_entries: 1000
        invalidate_on: ["file_creation", "file_deletion"]
      
      file_metadata:
        ttl: 600  # 10 minutes
        max_entries: 5000
        invalidate_on: ["file_modification"]
      
      file_content:
        ttl: 180  # 3 minutes
        max_entries: 100
        max_file_size: "1MB"
  
  handle_concurrent_access:
    description: "Manage concurrent file access safely"
    required: true
    mcp_server: "filesystem"
    implementation: |
      - Implement file locking mechanisms
      - Use filesystem.lock() for critical sections
      - Handle lock timeouts gracefully
      - Detect and resolve deadlocks

# FILESYSTEM MCP OPERATION PATTERNS
operation_patterns:
  atomic_file_updates:
    pattern: "temp_write_then_move"
    implementation: |
      async function atomicFileUpdate(filePath, content) {
        const tempPath = `${filePath}.tmp.${Date.now()}`;
        
        try {
          // Write to temporary file
          await filesystem.write(tempPath, content, { atomic: false });
          
          // Verify write success
          const written = await filesystem.read(tempPath);
          if (written !== content) {
            throw new Error('Write verification failed');
          }
          
          // Atomic move to final location
          await filesystem.move(tempPath, filePath, { atomic: true });
          
        } catch (error) {
          // Cleanup on failure
          await filesystem.delete(tempPath, { ignore_missing: true });
          throw error;
        }
      }
    
    use_cases:
      - Configuration file updates
      - Database file modifications
      - Application state persistence
      - Critical data saves
  
  streaming_operations:
    pattern: "chunked_streaming"
    implementation: |
      async function* streamFileChunks(filePath, chunkSize = 64 * 1024) {
        const handle = await filesystem.open(filePath, 'r');
        
        try {
          let offset = 0;
          while (true) {
            const chunk = await filesystem.read(handle, {
              offset: offset,
              length: chunkSize
            });
            
            if (chunk.length === 0) break;
            
            yield chunk;
            offset += chunk.length;
          }
        } finally {
          await filesystem.close(handle);
        }
      }
      
      async function streamWriteFile(filePath, dataGenerator) {
        const handle = await filesystem.open(filePath, 'w');
        
        try {
          for await (const chunk of dataGenerator) {
            await filesystem.write(handle, chunk);
          }
        } finally {
          await filesystem.close(handle);
        }
      }
    
    benefits:
      - "Constant memory usage regardless of file size"
      - "Better responsiveness for large operations"
      - "Ability to process files larger than available RAM"
      - "Progress tracking and cancellation support"
  
  batch_operations:
    pattern: "batched_directory_operations"
    implementation: |
      async function batchDirectoryOperations(operations) {
        const results = [];
        const batchSize = 10;
        
        for (let i = 0; i < operations.length; i += batchSize) {
          const batch = operations.slice(i, i + batchSize);
          const batchResults = await Promise.all(
            batch.map(op => filesystem[op.type](op.path, op.options))
          );
          results.push(...batchResults);
          
          // Add small delay between batches to prevent overwhelming
          if (i + batchSize < operations.length) {
            await sleep(10);
          }
        }
        
        return results;
      }
    
    optimization_benefits:
      - "Reduced MCP call overhead"
      - "Better resource utilization"
      - "Improved overall throughput"
      - "Rate limiting to prevent system overload"
  
  intelligent_prefetching:
    pattern: "predictive_file_loading"
    implementation: |
      class FilesystemPrefetcher {
        constructor() {
          this.cache = new Map();
          this.accessPatterns = new Map();
        }
        
        async read(filePath) {
          // Check cache first
          if (this.cache.has(filePath)) {
            this.recordAccess(filePath);
            return this.cache.get(filePath);
          }
          
          // Read from filesystem
          const content = await filesystem.read(filePath);
          this.cache.set(filePath, content);
          
          // Predict and prefetch related files
          this.prefetchRelated(filePath);
          
          return content;
        }
        
        prefetchRelated(filePath) {
          const related = this.predictRelatedFiles(filePath);
          related.forEach(path => this.prefetch(path));
        }
        
        predictRelatedFiles(filePath) {
          // Implement prediction logic based on:
          // - File patterns (same directory, similar names)
          // - Historical access patterns
          // - Project structure analysis
          return this.accessPatterns.get(filePath) || [];
        }
      }

# AGENT-SPECIFIC OPTIMIZATION PATTERNS
agent_optimizations:
  reader-agent:
    patterns:
      bulk_file_analysis:
        description: "Optimize reading multiple files for analysis"
        implementation: |
          async function analyzeProjectFiles(projectPath) {
            // Get file list first
            const files = await filesystem.list(projectPath, {
              recursive: true,
              pattern: '**/*.{js,ts,py,go,java}',
              exclude: ['node_modules', '.git', '__pycache__']
            });
            
            // Sort by size - process smaller files first for quicker feedback
            files.sort((a, b) => a.size - b.size);
            
            // Process in batches with different strategies based on size
            const results = [];
            for (const file of files) {
              if (file.size < 1024 * 1024) {  // < 1MB
                results.push(await filesystem.read(file.path));
              } else {  // >= 1MB
                results.push(await streamAnalyzeFile(file.path));
              }
            }
            
            return results;
          }
      
      directory_tree_caching:
        description: "Cache directory structures for repeated analysis"
        cache_duration: 300  # 5 minutes
        invalidation_triggers: ["file_system_events"]
  
  maker-agent:
    patterns:
      safe_code_generation:
        description: "Atomic code file generation with validation"
        implementation: |
          async function generateCodeFile(filePath, code) {
            const backupPath = `${filePath}.backup.${Date.now()}`;
            
            // Create backup if file exists
            const exists = await filesystem.exists(filePath);
            if (exists) {
              await filesystem.copy(filePath, backupPath);
            }
            
            try {
              // Atomic write
              await atomicFileUpdate(filePath, code);
              
              // Validate generated code (syntax check, linting, etc.)
              const isValid = await validateGeneratedCode(filePath);
              
              if (!isValid) {
                throw new Error('Generated code validation failed');
              }
              
              // Clean up backup on success
              if (exists) {
                await filesystem.delete(backupPath);
              }
              
            } catch (error) {
              // Restore backup on failure
              if (exists) {
                await filesystem.move(backupPath, filePath);
              } else {
                await filesystem.delete(filePath, { ignore_missing: true });
              }
              throw error;
            }
          }
      
      multi_file_refactoring:
        description: "Coordinate changes across multiple files atomically"
        implementation: |
          async function atomicMultiFileRefactor(changes) {
            const backups = new Map();
            const tempFiles = new Map();
            
            try {
              // Phase 1: Create backups and temp files
              for (const [filePath, newContent] of changes) {
                const backupPath = `${filePath}.backup.${Date.now()}`;
                const tempPath = `${filePath}.temp.${Date.now()}`;
                
                if (await filesystem.exists(filePath)) {
                  await filesystem.copy(filePath, backupPath);
                  backups.set(filePath, backupPath);
                }
                
                await filesystem.write(tempPath, newContent);
                tempFiles.set(filePath, tempPath);
              }
              
              // Phase 2: Validate all changes
              for (const [filePath, tempPath] of tempFiles) {
                const isValid = await validateFileChange(tempPath);
                if (!isValid) {
                  throw new Error(`Validation failed for ${filePath}`);
                }
              }
              
              // Phase 3: Atomic commits
              for (const [filePath, tempPath] of tempFiles) {
                await filesystem.move(tempPath, filePath);
              }
              
              // Phase 4: Cleanup backups
              for (const backupPath of backups.values()) {
                await filesystem.delete(backupPath);
              }
              
            } catch (error) {
              // Rollback: restore from backups and cleanup
              for (const [filePath, backupPath] of backups) {
                await filesystem.move(backupPath, filePath);
              }
              
              for (const tempPath of tempFiles.values()) {
                await filesystem.delete(tempPath, { ignore_missing: true });
              }
              
              throw error;
            }
          }
  
  debug-agent:
    patterns:
      log_file_streaming:
        description: "Stream large log files efficiently"
        implementation: |
          async function* streamLogLines(logFilePath, options = {}) {
            const { fromEnd = false, maxLines = Infinity, pattern = null } = options;
            
            if (fromEnd) {
              // Read from end of file (tail-like behavior)
              yield* streamLogFromEnd(logFilePath, maxLines);
            } else {
              // Read from beginning
              yield* streamLogFromBeginning(logFilePath, maxLines, pattern);
            }
          }
          
          async function* streamLogFromEnd(filePath, maxLines) {
            const stats = await filesystem.stat(filePath);
            const fileSize = stats.size;
            const chunkSize = 64 * 1024;
            let offset = Math.max(0, fileSize - chunkSize);
            let remainingLines = maxLines;
            
            const lines = [];
            
            while (offset >= 0 && remainingLines > 0) {
              const chunk = await filesystem.read(filePath, {
                offset: offset,
                length: Math.min(chunkSize, fileSize - offset)
              });
              
              const chunkLines = chunk.split('\n').reverse();
              
              for (const line of chunkLines) {
                if (line.trim() && remainingLines > 0) {
                  lines.unshift(line);
                  remainingLines--;
                }
              }
              
              offset -= chunkSize;
            }
            
            for (const line of lines) {
              yield line;
            }
          }
      
      performance_monitoring:
        description: "Monitor filesystem operation performance"
        metrics:
          - operation_duration
          - bytes_processed
          - cache_hit_rate
          - error_rate

# PERFORMANCE OPTIMIZATION STRATEGIES
performance:
  memory_management:
    strategies:
      stream_processing:
        description: "Process large files without loading entirely into memory"
        threshold: "10MB"
        implementation: "Use filesystem streaming APIs"
      
      buffer_pooling:
        description: "Reuse buffers to reduce garbage collection"
        pool_size: 10
        buffer_size: "64KB"
      
      memory_monitoring:
        description: "Monitor and limit memory usage during operations"
        max_memory_usage: "512MB"
        warning_threshold: "80%"
  
  io_optimization:
    strategies:
      async_operations:
        description: "Use asynchronous I/O to prevent blocking"
        concurrency_limit: 5
        timeout: "30s"
      
      batch_operations:
        description: "Group related operations to reduce syscall overhead"
        batch_size: 10
        batch_timeout: "100ms"
      
      compression:
        description: "Compress large files during storage/transfer"
        algorithms: ["gzip", "lz4", "zstd"]
        threshold: "1MB"
  
  caching_strategies:
    levels:
      l1_memory_cache:
        description: "In-memory cache for frequently accessed files"
        max_size: "100MB"
        ttl: "5 minutes"
        eviction_policy: "LRU"
      
      l2_disk_cache:
        description: "Disk-based cache for larger datasets"
        max_size: "1GB"
        ttl: "1 hour"
        eviction_policy: "LFU"
    
    invalidation:
      strategies:
        - time_based: "TTL expiration"
        - event_based: "File system change notifications"
        - size_based: "Cache size limits"
        - access_based: "LRU/LFU eviction"

# ERROR HANDLING AND RECOVERY
error_handling:
  retry_strategies:
    transient_errors:
      errors: ["EBUSY", "EAGAIN", "ETIMEDOUT"]
      max_attempts: 3
      backoff: "exponential"
      base_delay: "100ms"
    
    permanent_errors:
      errors: ["ENOENT", "EACCES", "ENOTDIR"]
      action: "immediate_failure"
      logging: "error_level"
  
  recovery_mechanisms:
    partial_failure_recovery:
      strategy: "Continue with successful operations"
      cleanup: "Remove incomplete artifacts"
      notification: "Report partial success status"
    
    transaction_rollback:
      strategy: "Restore previous state on failure"
      backup_retention: "24 hours"
      verification: "Validate rollback success"

# MONITORING AND METRICS
monitoring:
  performance_metrics:
    operation_latency:
      percentiles: [50, 90, 95, 99]
      targets:
        read_small_file: "< 10ms"
        read_large_file: "< 100ms"
        write_small_file: "< 15ms"
        write_large_file: "< 200ms"
    
    throughput:
      metrics:
        - operations_per_second
        - bytes_per_second
        - concurrent_operations
      
      targets:
        small_files: "> 100 ops/sec"
        large_files: "> 10 ops/sec"
        streaming: "> 50MB/sec"
  
  reliability_metrics:
    success_rate: "> 99.9%"
    error_recovery_rate: "> 95%"
    data_integrity: "100%"
  
  resource_utilization:
    memory_usage: "< 512MB peak"
    cpu_usage: "< 50% average"
    disk_space: "< 1GB cache"
    file_descriptors: "< 1000 concurrent"

# BEST PRACTICES AND GUIDELINES
best_practices:
  file_operations:
    - "Always use absolute paths"
    - "Validate file paths before operations"
    - "Handle permission errors gracefully"
    - "Implement proper cleanup in error scenarios"
    - "Use streaming for large files (> 10MB)"
    - "Implement atomic operations for critical files"
  
  performance:
    - "Cache frequently accessed directory listings"
    - "Use batch operations when possible"
    - "Monitor memory usage during large operations"
    - "Implement backpressure for streaming operations"
    - "Profile and optimize hot paths"
  
  reliability:
    - "Always implement error handling and recovery"
    - "Use checksums for critical data validation"
    - "Implement proper logging for debugging"
    - "Test edge cases and error scenarios"
    - "Monitor filesystem health and capacity"